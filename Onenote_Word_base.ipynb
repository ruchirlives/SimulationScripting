{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOhhTMiqg0p1SzfMaNfBhJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruchirlives/Python/blob/main/Onenote_Word_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet pandas\n",
        "%pip install --quiet python-docx beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRDGCBLJF6dG",
        "outputId": "0ee4e402-ade8-4c04-ad4f-40be3a6744ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m235.5/244.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converter classes"
      ],
      "metadata": {
        "id": "0OkgNo4FEfOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "from docx import Document\n",
        "from io import BytesIO\n",
        "from bs4 import BeautifulSoup\n",
        "from docx.oxml.ns import qn\n",
        "from docx.oxml import OxmlElement\n",
        "import os\n",
        "\n",
        "# Utility classes\n",
        "\n",
        "class OneNoteHandler:\n",
        "    def __init__(self, token, onenote_url):\n",
        "        self.token = token\n",
        "        self.base_url = \"https://graph.microsoft.com/v1.0/me/onenote/notebooks\"\n",
        "        self.request_url = self.get_notebook_request_url(onenote_url)\n",
        "\n",
        "    def get_notebook_request_url(self, webUrl):\n",
        "        getOnenote = f\"{self.base_url}/GetNotebookFromWebUrl\"\n",
        "        body = {'webUrl': webUrl}\n",
        "\n",
        "        response = requests.post(getOnenote, headers={'Authorization': f'Bearer {self.token}'}, json=body).json()\n",
        "        return response.get('self')\n",
        "\n",
        "    def list_notebook_sections(self):\n",
        "        listSections = f\"{self.request_url}/sections\"\n",
        "        response = requests.get(listSections, headers={'Authorization': f'Bearer {self.token}'}).json()\n",
        "\n",
        "        sections = response.get('value', [])\n",
        "        print(\"\\nSections:\")\n",
        "        for section in sections:\n",
        "            print(section['displayName'])\n",
        "\n",
        "        return sections\n",
        "\n",
        "    def get_pages(self, section_url):\n",
        "        listPages = f\"{section_url}/pages\"\n",
        "        response = requests.get(listPages, headers={'Authorization': f'Bearer {self.token}'}).json()\n",
        "        return response.get('value', [])\n",
        "\n",
        "    def get_page_content(self, contentUrl):\n",
        "        response = requests.get(contentUrl, headers={'Authorization': f'Bearer {self.token}'})\n",
        "        return response.content\n",
        "\n",
        "    def get_page(self, page_name, request_url):\n",
        "        sections = self.list_notebook_sections(request_url)\n",
        "        for section in sections:\n",
        "            pages = self.get_pages(section['self'])\n",
        "            for page in pages:\n",
        "                if page['title'] == page_name:\n",
        "                    print(f\"Found page: {page['title']}\")\n",
        "                    return page\n",
        "        return None\n",
        "\n",
        "    def get_page_text(self, page):\n",
        "        if page:\n",
        "            page_content = self.get_page_content(page['contentUrl'])\n",
        "            soup = BeautifulSoup(page_content, 'html.parser')\n",
        "            return soup.get_text()\n",
        "        return None\n",
        "\n",
        "    def get_page_html(self, page):\n",
        "        if page:\n",
        "            return self.get_page_content(page['contentUrl'])\n",
        "        return None\n",
        "\n",
        "    def write_page_text(self, page, text):\n",
        "        if page:\n",
        "            body = {\n",
        "                \"target\": \"body\",\n",
        "                \"action\": \"replace\",\n",
        "                \"content\": text\n",
        "            }\n",
        "            response = requests.patch(page['contentUrl'], headers={'Authorization': f'Bearer {self.token}'}, json=[body])\n",
        "            return response\n",
        "        return None\n",
        "\n",
        "class GraphAPIClient:\n",
        "    def __init__(self, token):\n",
        "        self.token = token\n",
        "\n",
        "    def get(self, url):\n",
        "        headers = {'Authorization': f'Bearer {self.token}'}\n",
        "        return requests.get(url, headers=headers)\n",
        "\n",
        "    def put(self, url, data):\n",
        "        headers = {'Authorization': f'Bearer {self.token}'}\n",
        "        return requests.put(url, headers=headers, data=data)\n",
        "\n",
        "class SharePointHandler:\n",
        "    def __init__(self, graph_client, hostname, site_name, doc_name):\n",
        "        self.graph_client = graph_client\n",
        "        self.hostname = hostname\n",
        "        self.site_name = site_name\n",
        "        self.doc_name = doc_name\n",
        "        self.site_id, self.item_id = self._get_ids()\n",
        "\n",
        "    def _get_ids(self):\n",
        "        request = f\"https://graph.microsoft.com/v1.0/sites/{self.hostname}:/sites/{self.site_name}\"\n",
        "        response = self.graph_client.get(request).json()\n",
        "        if 'id' not in response:\n",
        "            return None, None\n",
        "        site_id = response['id']\n",
        "\n",
        "        drive_url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drive/root/children\"\n",
        "        response = self.graph_client.get(drive_url).json()\n",
        "        doc_id = self._get_doc_id(response)\n",
        "\n",
        "        return site_id, doc_id\n",
        "\n",
        "    def _get_doc_id(self, response):\n",
        "        if 'value' not in response:\n",
        "            return None\n",
        "        for item in response['value']:\n",
        "            if item['name'] == self.doc_name:\n",
        "                return item['id']\n",
        "\n",
        "    def get_word_document(self):\n",
        "        get_word_url = f\"https://graph.microsoft.com/v1.0/sites/{self.site_id}/drive/items/{self.item_id}/content\"\n",
        "        response = self.graph_client.get(get_word_url)\n",
        "        if response.status_code == 200:\n",
        "            doc_stream = BytesIO(response.content)\n",
        "            return Document(doc_stream)\n",
        "        else:\n",
        "            print(f\"Failed to download document. Status code: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "    def upload_document(self, document, title):\n",
        "        upload_url = f\"https://graph.microsoft.com/v1.0/sites/{self.site_id}/drive/root:/FINAL/{title}:/content\"\n",
        "        with open(title, 'rb') as file:\n",
        "            response = self.graph_client.put(upload_url, file)\n",
        "            if response.status_code == 200:\n",
        "                print(\"File uploaded successfully.\")\n",
        "            else:\n",
        "                print(f\"Failed to upload file. Status code: {response.status_code}\")\n",
        "\n",
        "# Word document processing classes\n",
        "class WordDocumentProcessor:\n",
        "    def __init__(self, document, onenote_handler):\n",
        "        self.document = document\n",
        "        self.onenote_handler = onenote_handler\n",
        "\n",
        "    def delete_paragraph(self, paragraph):\n",
        "        p = paragraph._element\n",
        "        p.getparent().remove(p)\n",
        "        p._element = None\n",
        "\n",
        "    def replace_word_placeholder(self):\n",
        "        sections = self.onenote_handler.list_notebook_sections()\n",
        "        pages = self.onenote_handler.get_pages(sections[0]['self'])\n",
        "\n",
        "        for page in pages:\n",
        "            page_title = page['title']\n",
        "            page_content = self.onenote_handler.get_page_content(page['contentUrl'])\n",
        "            for paragraph in self.document.paragraphs[:]:\n",
        "                if not paragraph.style.name.startswith('Heading'):\n",
        "                    continue\n",
        "                if page_title in paragraph.text:\n",
        "                    print(\"Matched...\", page_title)\n",
        "                    self.document = self.merge_onenote(paragraph, page_content)\n",
        "        return self.document\n",
        "\n",
        "    def merge_onenote(self, target_paragraph, page_content):\n",
        "        soup = BeautifulSoup(page_content, 'html.parser')\n",
        "        tag = soup.html\n",
        "        new_paragraph = target_paragraph.insert_paragraph_before()\n",
        "        self.process_html_to_word(new_paragraph, tag)\n",
        "        self.delete_paragraph(target_paragraph)\n",
        "        return self.document\n",
        "\n",
        "    def process_html_to_word(self, paragraph, tag):\n",
        "        while tag is not None:\n",
        "            tag = self.get_next_tag(tag)\n",
        "            if tag is None:\n",
        "                break\n",
        "            self.translate_tag(paragraph, tag)\n",
        "\n",
        "    def get_next_tag(self, tag):\n",
        "        \"\"\"\n",
        "        Helper function to get the next valid tag in the paragraph.\n",
        "        \"\"\"\n",
        "        next_tag = tag.next_element\n",
        "\n",
        "        if next_tag is None:\n",
        "            print(\"No more elements\")\n",
        "            return None\n",
        "        elif next_tag.name is None:  # This is a text node or character\n",
        "            # Skip this and look for the next valid tag\n",
        "            return self.get_next_tag(next_tag)\n",
        "        elif next_tag.name == 'div':\n",
        "            # Skip the div tag, but continue checking next elements\n",
        "            return self.get_next_tag(next_tag)\n",
        "        elif next_tag.name in ['p', 'ul', 'ol', 'h1', 'h2', 'h3']:\n",
        "            # Found a valid tag, return it\n",
        "            return next_tag\n",
        "        else:\n",
        "            # Skip any other tags and keep looking\n",
        "            return self.get_next_tag(next_tag)\n",
        "\n",
        "    def translate_tag(self, paragraph, tag):\n",
        "        if isinstance(tag, str):\n",
        "            run = paragraph.add_run(tag)\n",
        "        elif tag.name == 'p':\n",
        "            new_paragraph = paragraph.insert_paragraph_before()\n",
        "            new_paragraph.add_run(tag.get_text())\n",
        "        elif tag.name in ['strong', 'b']:\n",
        "            run = paragraph.add_run(tag.get_text())\n",
        "            run.bold = True\n",
        "        elif tag.name == 'i':\n",
        "            run = paragraph.add_run(tag.get_text())\n",
        "            run.italic = True\n",
        "        elif tag.name.startswith('h') and tag.name in ['h1', 'h2', 'h3']:\n",
        "            new_paragraph = paragraph.insert_paragraph_before()\n",
        "            new_paragraph.style = f'Heading {tag.name[-1]}'\n",
        "            new_paragraph.add_run(tag.get_text())\n",
        "        elif tag.name == 'br':\n",
        "            paragraph.add_run().add_break()\n",
        "        elif tag.name in ['ul', 'ol']:\n",
        "            for li in tag.find_all('li'):\n",
        "                self.translate_tag(paragraph, li)\n",
        "        elif tag.name == 'li':\n",
        "            list_paragraph = paragraph.insert_paragraph_before(tag.get_text(), style='List Bullet')\n",
        "        elif tag.name == 'a':\n",
        "            self.add_hyperlink(paragraph, tag.get('href', ''), tag.get_text())\n",
        "\n",
        "    def add_hyperlink(self, paragraph, url, text):\n",
        "        run = paragraph.add_run(text)\n",
        "        r_id = paragraph.part.relate_to(url, \"http://schemas.openxmlformats.org/officeDocument/2006/relationships/hyperlink\", is_external=True)\n",
        "        hyperlink = OxmlElement('w:hyperlink')\n",
        "        hyperlink.set(qn('r:id'), r_id)\n",
        "        hyperlink_run = OxmlElement('w:r')\n",
        "        rPr = OxmlElement('w:rPr')\n",
        "        underline = OxmlElement('w:u')\n",
        "        underline.set(qn('w:val'), 'single')\n",
        "        color = OxmlElement('w:color')\n",
        "        color.set(qn('w:val'), '0000FF')\n",
        "        rPr.append(underline)\n",
        "        rPr.append(color)\n",
        "        hyperlink_run.append(rPr)\n",
        "        hyperlink_run.append(run._r)\n",
        "        hyperlink.append(hyperlink_run)\n",
        "        paragraph._element.append(hyperlink)\n",
        "\n",
        "# Main conversion class\n",
        "class SharePointDocumentConverter:\n",
        "    def __init__(self, graph_client, hostname, site_name, doc_name, onenote_url):\n",
        "        self.graph_client = graph_client\n",
        "        self.hostname = hostname\n",
        "        self.site_name = site_name\n",
        "        self.doc_name = doc_name\n",
        "        self.onenote_handler = OneNoteHandler(token, onenote_url)\n",
        "        self.sharepoint_handler = SharePointHandler(self.graph_client, self.hostname, self.site_name, self.doc_name)\n",
        "\n",
        "    def convert_document(self):\n",
        "        word_doc = self.sharepoint_handler.get_word_document()\n",
        "        if word_doc:\n",
        "            processor = WordDocumentProcessor(word_doc, self.onenote_handler)\n",
        "            updated_doc = processor.replace_word_placeholder()\n",
        "            title = updated_doc.paragraphs[0].text + \"_Draft.docx\"\n",
        "            updated_doc.save(title)\n",
        "            self.sharepoint_handler.upload_document(updated_doc, title)\n"
      ],
      "metadata": {
        "id": "oAMyXuTmED8B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}