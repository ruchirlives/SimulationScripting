{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1nvJLY5XsoPJe4t2QD3Sm4YdA4xDJafGA",
      "authorship_tag": "ABX9TyNQ86M5yjCaulKwIJlNXKOA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruchirlives/Python/blob/main/OpenAI_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load and run functions.py\n",
        "%pip install --quiet openai"
      ],
      "metadata": {
        "id": "U47JSOmN3Kns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d70f4bc-ba40-476e-a00e-f781afd8df52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.0/383.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "ZK90TGzA7dfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI"
      ],
      "metadata": {
        "id": "uneIZa5Uvoho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "from openai import OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OpenAI')\n",
        "\n",
        "def get_client():\n",
        "    client = OpenAI(\n",
        "        # This is the default and can be omitted\n",
        "        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        "    )\n",
        "    return client\n",
        "\n",
        "def get_openai(prompt):\n",
        "    client = get_client()\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=\"gpt-4o\",\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "def get_embeddings(text):\n",
        "    client = get_client()\n",
        "    response = client.embeddings.create(\n",
        "        input=text,\n",
        "        model=\"text-embedding-ada-002\",  # Adjust model name as necessary\n",
        "    )\n",
        "    embeddings = response.data[0].embedding  # Assuming a single input; adjust as needed\n",
        "    return embeddings\n",
        "\n",
        "def generate_summary(text):\n",
        "    prompt = f\"Please provide a summary of the following text in no more than 25 words:\\n\\n{text}\"\n",
        "    return get_openai(prompt)\n",
        "\n",
        "def get_category(text, categories, prompt):\n",
        "    client = get_client()\n",
        "\n",
        "    prompt = f\"{prompt} \\\n",
        "    Please classify the entry into one of the following categories, providing only the category by itself in the output. \\\n",
        "    \\n\\nCategories: {categories}\\n\\nText: {text}\"\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant who categorises stuff.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "vl9mC0B8YOoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraction"
      ],
      "metadata": {
        "id": "Ld4j34ZWVJWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "def generate_assertions(text):\n",
        "    client = OpenAI(\n",
        "        # This is the default and can be omitted\n",
        "        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"{ASSERTION_PROMPT} Here is the text: \" + text,\n",
        "            }\n",
        "        ],\n",
        "        model=\"gpt-4o\",\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def extract_assertions(text):\n",
        "\n",
        "    # generate assertions\n",
        "    try:\n",
        "        assertions = generate_assertions(text)\n",
        "    except Exception as e:\n",
        "        print('First try failed')\n",
        "        print(e)\n",
        "        try:\n",
        "            assertions = generate_assertions(text)\n",
        "        except Exception as e:\n",
        "            print('Second try failed')\n",
        "            print(e)\n",
        "            return None\n",
        "\n",
        "    # convert to list of dicts\n",
        "    data_string = assertions.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
        "    data_string = data_string.replace(\"'\", r\"\\'\")\n",
        "    assertions = ast.literal_eval(data_string)\n",
        "    return assertions\n",
        "\n",
        "# Test first row\n",
        "# assertions = process_assertions(blogs.iloc[82])"
      ],
      "metadata": {
        "id": "d9M8xvjDVJz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _List functions in module"
      ],
      "metadata": {
        "id": "KDwN-MPqwu_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list all functions defined in this module only\n",
        "\n",
        "import inspect\n",
        "import sys\n",
        "\n",
        "# Get the current module\n",
        "current_module = sys.modules[__name__]\n",
        "\n",
        "# List all functions and their arguments defined in this module\n",
        "functions_with_args = {func: str(inspect.signature(getattr(current_module, func)))\n",
        "                       for func in dir(current_module) if inspect.isfunction(getattr(current_module, func))}\n",
        "\n",
        "# Display the function names with their arguments\n",
        "for func, args in functions_with_args.items():\n",
        "    print(f\"{func}{args}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huc80fndwQC_",
        "outputId": "36136c2e-d047-465c-f767-71879d758b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['generate_summary', 'get_category', 'get_client', 'get_embeddings', 'get_openai']\n"
          ]
        }
      ]
    }
  ]
}